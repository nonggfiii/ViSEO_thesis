import os
from dotenv import load_dotenv
from pymongo import MongoClient
from sentence_transformers import SentenceTransformer
# from langchain_google_genai.chat_models import ChatGoogleGenerativeAI
# from langchain_community.chat_models import ChatOpenAI
from langchain_openai import ChatOpenAI 
from langchain.tools import tool
from langgraph.graph import StateGraph
import logging
from openai import OpenAI
import facebook
from flask import request,jsonify
import json
# Load environment variables
load_dotenv()
logging.basicConfig(level=logging.INFO)
print(os.getenv("FACEBOOK_PAGE_ACCESS_TOKEN"))
print(os.getenv("FACEBOOK_PAGE_ID"))
FACEBOOK_ACCESS_TOKEN = os.getenv("FACEBOOK_PAGE_ACCESS_TOKEN")
FACEBOOK_PAGE_ID = os.getenv("FACEBOOK_PAGE_ID")
logging.info(f"üì¢ DEBUG - FACEBOOK_ACCESS_TOKEN: {FACEBOOK_ACCESS_TOKEN}")
graph = facebook.GraphAPI(access_token=FACEBOOK_ACCESS_TOKEN)

# Connect to MongoDB
# MONGO_URI = os.getenv("MONGODB_URI")
# client = MongoClient(MONGO_URI)
# db = client["test_database"]
# indexed_collection = db["indexed_data"]

MONGO_URI = os.getenv("MONGODB_URI")
mongo_client = MongoClient(MONGO_URI)
db = mongo_client["eval_database"]
indexed_collection = db["indexed_data_openai_text3"]

client = OpenAI(
  api_key=os.environ['OPENAI_API_KEY'],  # this is also the default, it can be omitted
)

# Load embedding model
# embedding_model = SentenceTransformer("all-MiniLM-L6-v2")
# Initialize LLM for Supervisor
# GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
OPENAI_API = os.getenv("OPENAI_API_KEY")

from langchain.chat_models import ChatOpenAI

llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0.7,
    max_tokens=8000,
    openai_api_key=OPENAI_API,
    verbose=True
)
global_state = None

def embedding_model(text, model="text-embedding-3-small"):
    """
    Get embedding for a given text using OpenAI API.
    """
    try:
        response = client.embeddings.create(input=text, model=model)
        return response.data[0].embedding
    except Exception as e:
        logging.error(f"L·ªói khi t·∫°o embedding: {e}")
        return None


from langchain.schema import SystemMessage, HumanMessage

from pydantic import BaseModel
from typing import List

class GenerateSEOToolInput(BaseModel):
    product_name: str
    combined_chunks: str
    topic_title: str
    primary_keywords: List[str]
    secondary_keywords: List[str]
    platform: str = "website"

from langchain.schema import SystemMessage, HumanMessage


class Supervisor:
    def __init__(self):
        self.state = None

    def allocate_task(self):
        try:
            # üî• T·∫°o th√¥ng ƒëi·ªáp cho LLM
            system_message = SystemMessage(
                content="""
                You are a supervisor tasked with managing tasks for multiple agents.
                Your job is to allocate the correct tasks to the appropriate agents based on the user query.

                ### Agents Available:
                1. **Website SEO Writer Agent**: Writes SEO content tailored for websites.
                2. **Facebook SEO Writer Agent**: Writes SEO content optimized for Facebook posts.
                3. **Publisher Agent**: Handles publishing content to specific platforms.

                ### Output Format:
                - Thought: Describe the reasoning.
                - Action: Specify the agent to invoke.
                - Action Input: JSON format for agent inputs.

                ### Valid Actions:
                - write_seo_website
                - write_seo_facebook
                - publish_content

                If you cannot determine the next step, respond:
                - Thought: "I am unable to determine the next step with the given inputs."
                - Action: "None"
                """
            )

            user_message = HumanMessage(
                content=f"""
                Given the query: "{self.state.query}", determine the next action based on the agents available.
                Current State: {self.state.__dict__}.
                """
            )

            # üî• G·ªçi LLM ƒë·ªÉ quy·∫øt ƒë·ªãnh Agent n√†o c·∫ßn x·ª≠ l√Ω
            response = llm.generate([[system_message, user_message]])
            decision = response.generations[0][0].message.content.strip()
            logging.info(f"LLM Decision: {decision}")

            # üõ† Ki·ªÉm tra quy·∫øt ƒë·ªãnh v√† g·ªçi Agent ph√π h·ª£p
            if "write_seo_website" in decision:
                self.state.result = website_seo_writer_agent.write_seo(self.state)
                self.state.next_task = "end"

            elif "write_seo_facebook" in decision:
                self.state.result = facebook_seo_writer_agent.write_seo(self.state)

                # ‚úÖ ƒê·∫£m b·∫£o `state.context` kh√¥ng b·ªã r·ªóng
                if self.state.context is None:
                    self.state.context = {}

                # ‚úÖ L∆∞u n·ªôi dung v√†o `state.context`
                self.state.context["last_facebook_post"] = {
                    "content": self.state.result,
                    "product_name": self.state.query
                }
                logging.info(f"‚úÖ DEBUG: ƒê√£ l∆∞u b√†i vi·∫øt Facebook v√†o state.context: {self.state.context}")

                self.state.next_task = "end"

            elif "publish_content" in decision:
                # ‚úÖ Ki·ªÉm tra `state.context["last_facebook_post"]`
                if self.state.context is None:
                    logging.error("‚ùå DEBUG: `state.context` b·ªã m·∫•t ho·∫∑c kh√¥ng t·ªìn t·∫°i.")
                    self.state.result = "Kh√¥ng c√≥ n·ªôi dung n√†o ƒë·ªÉ ƒëƒÉng l√™n Facebook. H√£y y√™u c·∫ßu t·∫°o n·ªôi dung tr∆∞·ªõc."
                    self.state.next_task = "end"
                    return

                last_post = self.state.context.get("last_facebook_post")

                if last_post:
                    logging.info("üì¢ DEBUG: ƒêang ƒëƒÉng b√†i l√™n Facebook...")

                    # ‚úÖ G·ªçi `PublisherAgent` v·ªõi ƒë·∫ßy ƒë·ªß `state.context`
                    self.state.result = publisher_agent.publish_content(self.state)
                else:
                    logging.error("‚ùå DEBUG: Kh√¥ng c√≥ n·ªôi dung n√†o ƒë·ªÉ ƒëƒÉng l√™n Facebook.")
                    self.state.result = "Kh√¥ng c√≥ n·ªôi dung n√†o ƒë·ªÉ ƒëƒÉng l√™n Facebook. H√£y y√™u c·∫ßu t·∫°o n·ªôi dung tr∆∞·ªõc."

                self.state.next_task = "end"

            else:
                logging.error(f"Invalid task returned by LLM: {decision}")
                self.state.result = "Invalid task."
                self.state.next_task = "end"

        except Exception as e:
            logging.error(f"‚ùå Error in allocate_task: {e}")
            self.state.result = "An error occurred."
            self.state.next_task = "end"

    
    def run(self, query):
        """Gi·ªØ nguy√™n context khi ch·∫°y Supervisor nhi·ªÅu l·∫ßn."""
        if self.state is None:
            self.state = AgentState(query=query, next_task="start")
        else:
            self.state.query = query
            self.state.next_task = "start"

        logging.info(f"üîÑ DEBUG - State khi b·∫Øt ƒë·∫ßu run: {self.state.__dict__}")

        while self.state.next_task != "end":
            self.allocate_task()

            if not self.state.result:
                logging.error("‚ùå No result returned by the agent.")
                self.state.next_task = "end"



# Define AgentState
class AgentState:
    def __init__(self, query, next_task, context=None):
        self.query = query
        self.next_task = next_task
        self.result = None
        self.context = context if context else {}  


@tool("vector_search")
def vector_search(query):
    """
    Truy v·∫•n MongoDB Atlas b·∫±ng vector search ƒë·ªÉ l·∫•y `topic_title`, `primary_keywords`, `secondary_keywords`, 
    v√† `chunk_texts` li√™n quan ƒë·∫øn s·∫£n ph·∫©m.
    """
    if not query:
        logging.error("‚ùå Kh√¥ng c√≥ query ƒë·∫ßu v√†o cho vector search.")
        return {}

    try:
        # üîπ Encode `query` th√†nh vector
        query_vector = embedding_model(query)

        # üîπ 1Ô∏è‚É£ T√¨m topic b·∫±ng `topic_embedding`
        topic_pipeline = [
            {
                "$vectorSearch": {
                    "index": "product_index_openai_text3",
                    "path": "topic_embedding",
                    "queryVector": query_vector,
                    "numCandidates": 5,
                    "limit": 1
                }
            },
            {
                "$project": {
                    "_id": 0,
                    "product_name": 1,
                    "topic_title": 1,
                    "primary_keywords": 1,
                    "secondary_keywords": 1,
                    "score": {"$meta": "vectorSearchScore"}
                }
            }
        ]

        topic_results = list(indexed_collection.aggregate(topic_pipeline))

        # ‚ùå N·∫øu kh√¥ng t√¨m th·∫•y topic, tr·∫£ v·ªÅ l·ªói
        if not topic_results:
            logging.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y topic ph√π h·ª£p.")
            return {}

        # ‚úÖ Ch·ªçn topic c√≥ ƒëi·ªÉm cao nh·∫•t
        selected_topic = topic_results[0]
        matching_product = selected_topic.get("product_name", "Kh√¥ng x√°c ƒë·ªãnh")
        topic_title = selected_topic.get("topic_title", f"H∆∞·ªõng d·∫´n v·ªÅ {matching_product}")
        primary_keywords = selected_topic.get("primary_keywords", []) or [matching_product]
        secondary_keywords = selected_topic.get("secondary_keywords", []) or []

        logging.info(f"üü¢ T√¨m th·∫•y topic: {topic_title} cho s·∫£n ph·∫©m {matching_product}")

        # üîπ 2Ô∏è‚É£ T√¨m `chunk_texts` trong `chunk_embedding` theo `product_name`
        chunk_pipeline = [
            {
                "$match": {
                    "product_name": matching_product
                }
            },
            {
                "$project": {
                    "_id": 0,
                    "chunk": 1
                }
            }
        ]

        chunk_results = list(indexed_collection.aggregate(chunk_pipeline))

        # ‚úÖ N·∫øu kh√¥ng t√¨m th·∫•y chunk, g√°n gi√° tr·ªã m·∫∑c ƒë·ªãnh
        if not chunk_results:
            logging.warning(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y chunk n√†o cho s·∫£n ph·∫©m {matching_product}.")
            chunk_texts = "Kh√¥ng c√≥ d·ªØ li·ªáu s·∫£n ph·∫©m chi ti·∫øt."
        else:
            # Ki·ªÉm tra t·ª´ng document xem c√≥ `chunk` kh√¥ng
            valid_chunks = [result.get("chunk", "") for result in chunk_results if "chunk" in result]

            if not valid_chunks:
                logging.warning(f"‚ö†Ô∏è Kh√¥ng c√≥ document n√†o ch·ª©a `chunk` cho s·∫£n ph·∫©m {matching_product}.")
                chunk_texts = "Kh√¥ng c√≥ d·ªØ li·ªáu s·∫£n ph·∫©m chi ti·∫øt."
            else:
                chunk_texts = " ".join(valid_chunks)

        logging.info(f"‚úÖ T√¨m th·∫•y {len(chunk_results)} chunks.")

        # üîπ 3Ô∏è‚É£ Tr·∫£ v·ªÅ context ƒë·∫ßy ƒë·ªß
        final_context = {
            "product_name": matching_product,
            "topic_title": topic_title,
            "primary_keywords": primary_keywords,
            "secondary_keywords": secondary_keywords,
            "chunk_texts": chunk_texts
        }

        logging.info(f"‚úÖ Final context retrieved for {matching_product}")
        return final_context

    except Exception as e:
        logging.error(f"‚ùå L·ªói trong vector search: {e}")
        return {}

def search_topics_by_product_name(query):
    try:
        pipeline = [
            {
                "$search": {
                    "index": "topic_title_index",
                    "autocomplete": {
                        "query": query,
                        "path": "product_name"
                    }
                }
            },
            {
                "$match": { "topic_title": { "$exists": True } }
            },
            {
                "$project": {
                    "_id": 0,
                    "product_name": 1,
                    "topic_title": 1
                }
            }
        ]

        results = list(indexed_collection.aggregate(pipeline))

        if not results:
            return {"message": f"Kh√¥ng t√¨m th·∫•y topic n√†o cho s·∫£n ph·∫©m: {query}"}, 404

        return {"topics": results}, 200

    except Exception as e:
        return {"error": str(e)}, 500

import requests

SERPAPI_KEY = os.getenv("SERPAPI_KEY")

def fetch_product_images(product_name, num_images=4):
    """
    Fetch product images from SerpApi Google Images API.
    Returns a list of image URLs (up to `num_images`).
    """
    try:
        url = "https://serpapi.com/search.json"
        params = {
            "engine": "google_images",
            "q": product_name,
            "api_key": SERPAPI_KEY,
            "num": num_images  # L·∫•y s·ªë l∆∞·ª£ng ·∫£nh c·∫ßn
        }
        response = requests.get(url, params=params)
        response.raise_for_status()
        data = response.json()

        # L·ªçc ra t·ªëi ƒëa `num_images` ·∫£nh
        image_urls = [
            img["original"] for img in data.get("images_results", [])[:num_images]
        ]

        if image_urls:
            logging.info(f"Found {len(image_urls)} images for {product_name}.")
            return image_urls
        else:
            logging.warning(f"No images found for {product_name}.")
            return []

    except Exception as e:
        logging.error(f"Error fetching images for {product_name}: {e}")
        return []


@tool("generate_seo_tool", args_schema=GenerateSEOToolInput)
def generate_seo_tool(
    product_name: str,
    combined_chunks: str,
    topic_title: str,
    primary_keywords: List[str],
    secondary_keywords: List[str],
    platform: str = "website",
):
    """Generate SEO-optimized content for Website or Facebook based on product data and topics."""
        # **üîé N·∫øu kh√¥ng t√¨m th·∫•y Topic, AI s·∫Ω t·ª± ƒë·ªông sinh Topic & Keywords**
    if not topic_title:
        logging.warning(f"Kh√¥ng t√¨m th·∫•y topic ph√π h·ª£p cho s·∫£n ph·∫©m {product_name}. AI s·∫Ω t·ª± sinh topic...")

        generate_topic_prompt = f"""
        B·∫°n l√† m·ªôt chuy√™n gia SEO. H√£y t·∫°o m·ªôt **Topic ti√™u ƒë·ªÅ v√† b·ªô t·ª´ kh√≥a** ph√π h·ª£p ƒë·ªÉ t·ªëi ∆∞u SEO cho s·∫£n ph·∫©m **{product_name}**.

        - **Topic Title**: Ph·∫£i h·∫•p d·∫´n, ch·ª©a th√¥ng tin s·∫£n ph·∫©m, chu·∫©n SEO.
        - **Primary Keywords**: 3-5 t·ª´ kh√≥a ch√≠nh, c√≥ l∆∞·ª£t t√¨m ki·∫øm cao.
        - **Secondary Keywords**: 3-5 t·ª´ kh√≥a b·ªï tr·ª£.

        Xu·∫•t k·∫øt qu·∫£ d∆∞·ªõi JSON:
        ```json
        {{
            "topic_title": "T√™n topic...",
            "primary_keywords": ["keyword1", "keyword2", "keyword3"],
            "secondary_keywords": ["related1", "related2", "related3"]
        }}
        ```
        """
        human_message = HumanMessage(content=generate_topic_prompt)
        response = llm.invoke([human_message])

        try:
            generated_data = json.loads(response.content.strip("```json").strip())
            topic_title = generated_data["topic_title"]
            primary_keywords = generated_data["primary_keywords"]
            secondary_keywords = generated_data["secondary_keywords"]
            logging.info(f"üìå AI sinh topic m·ªõi: {topic_title}")
        except Exception as e:
            logging.error(f"‚ùå L·ªói khi sinh topic b·∫±ng AI: {e}")
            topic_title = f"T·ªïng Quan v·ªÅ {product_name}"  # Fallback
            primary_keywords = [product_name]
            secondary_keywords = []

    # **üîé Fetch product images**
    product_images = fetch_product_images(product_name)
    image_section = "\n\n".join([
        f"![{product_name} Image {i+1}]({img})"
        for i, img in enumerate(product_images)
    ]) if product_images else "Kh√¥ng c√≥ ·∫£nh s·∫£n ph·∫©m."

    try:
        # ‚úÖ **Generate Website SEO Content**
        if platform == "website":
            prompt = f"""
            B·∫°n l√† m·ªôt chuy√™n gia vi·∫øt n·ªôi dung SEO. H√£y t·∫°o b√†i vi·∫øt t·ªëi ∆∞u h√≥a SEO v·ªÅ s·∫£n ph·∫©m **{product_name}**, c√≥ ƒë·ªô d√†i kho·∫£ng **2500-3000 t·ª´**, d·ª±a tr√™n th√¥ng tin sau:

            **Topic**: {topic_title}
            **Primary Keywords**: {', '.join(primary_keywords)}
            **Secondary Keywords**: {', '.join(secondary_keywords)}

            **Th√¥ng tin s·∫£n ph·∫©m**: {combined_chunks}
            **H√¨nh ·∫£nh s·∫£n ph·∫©m**: {image_section}
            
            ## **Y√™u c·∫ßu t·ªëi ∆∞u SEO:**
            1. **T·ª´ kh√≥a quan tr·ªçng c·∫ßn xu·∫•t hi·ªán**:
            - **Primary Keywords**: Xu·∫•t hi·ªán **5-7 l·∫ßn**.
            - **Secondary Keywords**: Xu·∫•t hi·ªán **3-5 l·∫ßn**.
            - T·ª´ kh√≥a ph·∫£i ƒë∆∞·ª£c s·ª≠ d·ª•ng **t·ª± nhi√™n**, kh√¥ng nh·ªìi nh√©t.

            2. **H∆∞·ªõng d·∫´n ch√®n t·ª´ kh√≥a**:
            - T·ª´ kh√≥a ch√≠nh **ph·∫£i c√≥ trong**:
                - **Ti√™u ƒë·ªÅ ch√≠nh (H1)**, **Meta Description**, **URL**, **10% ƒë·∫ßu b√†i vi·∫øt**.
                - **Ti√™u ƒë·ªÅ ph·ª• (H2, H3, H4)**, **alt c·ªßa h√¨nh ·∫£nh**, v√† ph√¢n b·ªï t·ª± nhi√™n trong n·ªôi dung (**1% - 1.5% m·∫≠t ƒë·ªô t·ª´ kh√≥a**).
            - **Kh√¥ng l·∫∑p l·∫°i t·ª´ kh√≥a y nguy√™n qu√° nhi·ªÅu l·∫ßn**, h√£y d√πng t·ª´ ƒë·ªìng nghƒ©a, t·ª´ kh√≥a ph·ª•.

            ---

            ## **C·∫•u tr√∫c b√†i vi·∫øt chu·∫©n SEO:**
            1. **M·ªü b√†i**: 
            - Thu h√∫t ng∆∞·ªùi ƒë·ªçc, gi·ªõi thi·ªáu t·ªïng quan s·∫£n ph·∫©m.
            - Ch·ª©a **Primary Keyword** trong c√¢u ƒë·∫ßu ti√™n.

            2. **Th√¢n b√†i**:
            - M√¥ t·∫£ **ƒë·∫∑c ƒëi·ªÉm, c√¥ng nƒÉng, l·ª£i √≠ch**.
            - **FAQ (C√¢u h·ªèi th∆∞·ªùng g·∫∑p)**: S·ª≠ d·ª•ng c√¢u h·ªèi c√≥ t·ª´ kh√≥a ph·ª•.
            - **H√¨nh ·∫£nh** (s·ª≠ d·ª•ng Markdown):
                - üñº **Sau m·ªü b√†i**  
                - üñº **Gi·ªØa b√†i vi·∫øt**  
                - üñº **·ªû ph·∫ßn ƒë·∫∑c ƒëi·ªÉm s·∫£n ph·∫©m**  
                - üñº **Tr∆∞·ªõc ph·∫ßn k·∫øt lu·∫≠n**  

            3. **K·∫øt b√†i**:
            - K√™u g·ªçi h√†nh ƒë·ªông (CTA) r√µ r√†ng.

            ---

            ## **Chu·∫©n E-E-A-T trong SEO Content:**
            - **Tr·∫£i nghi·ªám (Experience)**: Th√™m v√≠ d·ª• th·ª±c t·∫ø v·ªÅ s·∫£n ph·∫©m.  
            - **Chuy√™n m√¥n (Expertise)**: N·ªôi dung ch√≠nh x√°c, nh·∫•n m·∫°nh ƒë·∫∑c ƒëi·ªÉm k·ªπ thu·∫≠t.  
            - **T√≠nh th·∫©m quy·ªÅn (Authoritativeness)**: D·∫´n ch·ª©ng t·ª´ ngu·ªìn tin c·∫≠y.  
            - **ƒê·ªô tin c·∫≠y (Trustworthiness)**: Tr√¨nh b√†y r√µ r√†ng, kh√¥ng g√¢y hi·ªÉu l·∫ßm.

            ---

            ## **ƒê·ªãnh d·∫°ng & K·ªπ thu·∫≠t SEO:**
            - **Page Title**: B·∫Øt bu·ªôc ch·ª©a **Primary Keyword**, kh√¥ng qu√° **60 k√Ω t·ª±**.
            - **Meta Description**: T√≥m t·∫Øt l·ª£i √≠ch s·∫£n ph·∫©m, ch·ª©a t·ª´ kh√≥a, **t·ªëi ƒëa 160 k√Ω t·ª±**.
            - **URL**: Ng·∫Øn g·ªçn, kh√¥ng d√†i qu√° **75 k√Ω t·ª±**.
            - **Markdown**:
            - `#` cho **H1**, `##` cho **H2**, `###` cho **H3**.
            - **In ƒë·∫≠m** (`**bold**`) cho t·ª´ kh√≥a quan tr·ªçng.
            - D√πng `-` cho danh s√°ch g·∫°ch ƒë·∫ßu d√≤ng, `1.` cho danh s√°ch s·ªë th·ª© t·ª±.
            - D√πng blockquote (`>`) cho c√°c tr√≠ch d·∫´n quan tr·ªçng.

            ---

            ## **L∆∞u √Ω quan tr·ªçng**:
            - **Xu·∫•t ra b√†i vi·∫øt ho√†n ch·ªânh d∆∞·ªõi ƒë·ªãnh d·∫°ng Markdown**.
            - B√†i vi·∫øt ph·∫£i bao g·ªìm th√¥ng tin Gi√° b√°n s·∫£n ph·∫©m, Ch√≠nh s√°ch b·∫£o h√†nh ƒë√£ ƒë∆∞·ª£c cung c·∫•p, v√† ƒê·∫£m b·∫£o h√†ng ch√≠nh h√£ng.
            - Ch√®n th√™m m·∫•y c√¢u ki·ªÉu nh∆∞ "H√£y ƒë·∫øn v·ªõi [Ch·ªó n√†y cho ng∆∞·ªùi d√πng t·ª± nh·∫≠p t√™n c·ª≠a h√†ng] ƒë·ªÉ mua s·∫£n ph·∫©m" ·ªü cu·ªëi b√†i (b·∫°n c√≥ th·ªÉ vi·∫øt c√¢u kh√°c c√πng √Ω nghƒ©a nh∆∞ng hay h∆°n).
            - **Kh√¥ng th√™m ph·∫ßn gi·∫£i th√≠ch, ch·ªâ xu·∫•t n·ªôi dung b√†i vi·∫øt**.
            - **Tu√¢n th·ªß ti√™u chu·∫©n SEO v√† E-E-A-T**, ƒë·∫∑c bi·ªát n·∫øu s·∫£n ph·∫©m thu·ªôc lƒ©nh v·ª±c **YMYL (Your Money, Your Life)** nh∆∞ s·ª©c kh·ªèe, t√†i ch√≠nh.
            """

        
        # ‚úÖ **Generate Facebook Ad Content**
        elif platform == "facebook":
            prompt = f"""
            Vi·∫øt n·ªôi dung **qu·∫£ng c√°o Facebook** thu h√∫t, t·ªëi ∆∞u SEO cho s·∫£n ph·∫©m **{product_name}**, d·ª±a tr√™n c√°c th√¥ng tin sau:

            **Topic**: {topic_title}
            **Primary Keywords**: {', '.join(primary_keywords)}
            **Secondary Keywords**: {', '.join(secondary_keywords)}

            **Th√¥ng tin s·∫£n ph·∫©m**: {combined_chunks}
            **H√¨nh ·∫£nh s·∫£n ph·∫©m**: {image_section}

                    ### üéØ **Y√™u c·∫ßu quan tr·ªçng:**
            - **Gi·ªçng vƒÉn:**  
            - Ng·∫Øn g·ªçn, h·∫•p d·∫´n, th√¢n thi·ªán.  
            - T·∫≠p trung v√†o vi·ªác kh∆°i g·ª£i c·∫£m x√∫c, k√≠ch th√≠ch h√†nh ƒë·ªông.  
            - **T·ª´ kh√≥a SEO:**  
            - T·ª´ kh√≥a ch√≠nh: **{', '.join(primary_keywords)}**.
            - Xu·∫•t hi·ªán √≠t nh·∫•t **3 l·∫ßn** trong b√†i.  
            - Tr√¨nh b√†y t·ª± nhi√™n, tr√°nh nh·ªìi nh√©t t·ª´ kh√≥a.  
            - **ƒê·ªô d√†i:**  
            - D∆∞·ªõi **300 t·ª´**, tr√°nh di·ªÖn ƒë·∫°t lan man.  
            - M·ªói ƒëo·∫°n kh√¥ng qu√° **2-3 c√¢u** ƒë·ªÉ ƒë·∫£m b·∫£o d·ªÖ ƒë·ªçc.  

            ---

            ### üéØ **C·∫•u tr√∫c n·ªôi dung:**
            1. **Ti√™u ƒë·ªÅ (Headline) ng·∫Øn g·ªçn & n·ªïi b·∫≠t:**  
            - T·ªëi ƒëa **60 k√Ω t·ª±**.  
            - B·∫Øt ƒë·∫ßu v·ªõi **t·ª´ kh√≥a ch√≠nh** v√† t·∫≠p trung v√†o **l·ª£i √≠ch ch√≠nh** c·ªßa s·∫£n ph·∫©m.  
            - V√≠ d·ª•: `"iPhone 16 Pro ‚Äì ƒê·ªânh Cao C√¥ng Ngh·ªá Hi·ªán ƒê·∫°i!"`  

            2. **M·ªü b√†i h·∫•p d·∫´n:**  
            - G·ª£i m·ªü s·ª± t√≤ m√≤ ho·∫∑c n√™u v·∫•n ƒë·ªÅ c·∫ßn gi·∫£i quy·∫øt.  
            - Ch√®n t·ª´ kh√≥a ch√≠nh **ngay ƒë·∫ßu ƒëo·∫°n**.  
            - V√≠ d·ª•: `"B·∫°n ƒëang t√¨m ki·∫øm m·ªôt chi·∫øc smartphone cao c·∫•p v·ªõi camera ƒë·ªânh cao v√† hi·ªáu su·∫•t m·∫°nh m·∫Ω? H√£y kh√°m ph√° ngay iPhone 16 Pro!"`  

            3. **Th√¢n b√†i ‚Äì Nh·∫•n m·∫°nh l·ª£i √≠ch ch√≠nh:**  
            - N√™u **3 l·ª£i √≠ch ch√≠nh** n·ªïi b·∫≠t nh·∫•t.  
            - D√πng **d·∫•u g·∫°ch ƒë·∫ßu d√≤ng** ho·∫∑c bi·ªÉu t∆∞·ª£ng c·∫£m x√∫c ƒë·ªÉ tƒÉng ƒëi·ªÉm nh·∫•n.  
            - V√≠ d·ª•:  
                - ‚úÖ **Camera 48MP s·∫Øc n√©t, ch·ª•p ·∫£nh chuy√™n nghi·ªáp.**  
                - ‚úÖ **Chip A18 Pro m·∫°nh m·∫Ω, x·ª≠ l√Ω m∆∞·ª£t m√† m·ªçi t√°c v·ª•.**  
                - ‚úÖ **M√†n h√¨nh Super Retina XDR s·ªëng ƒë·ªông, hi·ªÉn th·ªã ho√†n h·∫£o.**  

            4. **K√™u g·ªçi h√†nh ƒë·ªông m·∫°nh m·∫Ω (CTA):**  
            - ƒê∆∞a ra **h√†nh ƒë·ªông r√µ r√†ng** & **h·∫•p d·∫´n**.  
            - V√≠ d·ª•:  
                - üëâ **ƒê·∫∑t h√†ng ngay h√¥m nay v√† nh·∫≠n ∆∞u ƒë√£i ƒë·ªôc quy·ªÅn!**  
                - üéØ **Mua ngay t·∫°i [Link b√°n h√†ng]** ƒë·ªÉ tr·∫£i nghi·ªám c√¥ng ngh·ªá ƒë·ªânh cao.  

            ---

            ### üéØ **T·ªëi ∆∞u E-E-A-T tr√™n Facebook:**
            - **Experience:** Ch√®n v√≠ d·ª• th·ª±c t·∫ø ho·∫∑c ƒë√°nh gi√° t·ª´ kh√°ch h√†ng.  
            - V√≠ d·ª•: `"H∆°n 1000 kh√°ch h√†ng ƒë√£ tr·∫£i nghi·ªám v√† h√†i l√≤ng!"`  
            - **Expertise:** ƒê∆∞a ra **th√¥ng s·ªë k·ªπ thu·∫≠t ng·∫Øn g·ªçn** gi√∫p tƒÉng ƒë·ªô tin c·∫≠y.  
            - **Authoritativeness:** ƒê·∫£m b·∫£o th√¥ng tin t·ª´ **ngu·ªìn ch√≠nh h√£ng** (Apple ho·∫∑c c·ª≠a h√†ng uy t√≠n).  
            - **Trustworthiness:** ƒê·ªÅ c·∫≠p **gi√° c·∫£**, **ch√≠nh s√°ch b·∫£o h√†nh**, v√† **ƒë·∫£m b·∫£o h√†ng ch√≠nh h√£ng**.  

            ---

            ### üéØ **L∆∞u √Ω k·ªπ thu·∫≠t SEO:**
            - T·ª´ kh√≥a ch√≠nh **ph·∫£i xu·∫•t hi·ªán trong**:  
            - **Ti√™u ƒë·ªÅ, M·ªü b√†i, Th√¢n b√†i v√† CTA.**  
            - **Tr√°nh spam t·ª´ kh√≥a** v√† ƒë·∫£m b·∫£o gi·ªçng vƒÉn **t·ª± nhi√™n**.  
            - N·ªôi dung n√™n s·ª≠ d·ª•ng **Markdown** ho·∫∑c bi·ªÉu t∆∞·ª£ng c·∫£m x√∫c ƒë·ªÉ tƒÉng ƒëi·ªÉm nh·∫•n.  
            - Ch√®n h√¨nh ·∫£nh s·∫£n ph·∫©m ƒë·ªÉ tƒÉng s·ª± h·∫•p d·∫´n.
            ---

            ### **K·∫øt qu·∫£ y√™u c·∫ßu:**
            - Xu·∫•t n·ªôi dung d∆∞·ªõi **300 t·ª´**.  
            - ƒê·∫£m b·∫£o c·∫•u tr√∫c ti√™u ƒë·ªÅ, m·ªü b√†i, th√¢n b√†i, CTA r√µ r√†ng. 
            - Th√™m c√°c hashtag hi·ªáu qu·∫£ 
            - **Kh√¥ng gi·∫£i th√≠ch, ch·ªâ xu·∫•t n·ªôi dung ho√†n ch·ªânh.**  
            ### **Xu·∫•t n·ªôi dung ho√†n ch·ªânh d∆∞·ªõi 300 t·ª´.**
            """

        else:
            raise ValueError("Invalid platform")

        # **üß† G·ªçi LLM ƒë·ªÉ sinh n·ªôi dung**
        human_message = HumanMessage(content=prompt)
        response = llm.invoke([human_message])
        return response.content.strip()

    except Exception as e:
        logging.error(f"Error in generate_seo_tool: {e}")
        return f"Error: {e}"

class WebsiteSEOWriterAgent:
    def write_seo(self, state):
        try:
            # üîé G·ªçi `vector_search()` ƒë·ªÉ l·∫•y d·ªØ li·ªáu
            context_data = vector_search(state.query)

            # ‚ùå N·∫øu kh√¥ng c√≥ k·∫øt qu·∫£ t·ª´ vector_search, tr·∫£ v·ªÅ l·ªói
            if not context_data:
                logging.error("‚ùå Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ n√†o t·ª´ vector_search.")
                state.result = "Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ n√†o ph√π h·ª£p v·ªõi t·ª´ kh√≥a."
                state.next_task = "end"
                return state.result

            # üè∑Ô∏è L·∫•y th√¥ng tin t·ª´ context_data
            product_name = context_data.get("product_name", state.query) or "Kh√¥ng x√°c ƒë·ªãnh"
            topic_title = context_data.get("topic_title", f"H∆∞·ªõng d·∫´n v·ªÅ {product_name}")
            primary_keywords = context_data.get("primary_keywords", []) or [product_name]
            secondary_keywords = context_data.get("secondary_keywords", []) or []
            combined_chunks = context_data.get("chunk_texts", "Kh√¥ng c√≥ d·ªØ li·ªáu chi ti·∫øt.")

            # üîé Ki·ªÉm tra d·ªØ li·ªáu tr∆∞·ªõc khi g·ªçi API ƒë·ªÉ tr√°nh l·ªói thi·∫øu tham s·ªë
            if not topic_title:
                logging.error("‚ùå `topic_title` b·ªã thi·∫øu!")
                topic_title = f"H∆∞·ªõng d·∫´n v·ªÅ {product_name}"

            if not primary_keywords:
                logging.error("‚ùå `primary_keywords` b·ªã thi·∫øu!")
                primary_keywords = [product_name]

            if not secondary_keywords:
                logging.error("‚ùå `secondary_keywords` b·ªã thi·∫øu!")
                secondary_keywords = []

            if not combined_chunks:
                logging.warning("‚ö†Ô∏è `combined_chunks` tr·ªëng, n·ªôi dung c√≥ th·ªÉ b·ªã thi·∫øu.")
                combined_chunks = "Kh√¥ng c√≥ d·ªØ li·ªáu s·∫£n ph·∫©m chi ti·∫øt."

            # üõ† Debug log ƒë·ªÉ ki·ªÉm tra d·ªØ li·ªáu truy·ªÅn v√†o API
            logging.info(f"‚úÖ G·ªçi generate_seo_tool v·ªõi:")
            logging.info(f"üîπ Topic Title: {topic_title}")
            logging.info(f"üîπ Primary Keywords: {primary_keywords}")
            logging.info(f"üîπ Secondary Keywords: {secondary_keywords}")
            logging.info(f"üîπ Combined Chunks (Preview): {combined_chunks[:100]}...")  # Gi·ªõi h·∫°n log

            # üîπ **Fix c√°ch g·ªçi `generate_seo_tool()`**
            response = generate_seo_tool.invoke({
                "product_name": product_name,
                "combined_chunks": combined_chunks,
                "topic_title": topic_title,
                "primary_keywords": primary_keywords,
                "secondary_keywords": secondary_keywords,
                "platform": "website"
            })

            return response

        except ValueError as e:
            logging.error(f"‚ùå L·ªói ValueError: {e}")
            state.result = str(e)
            state.next_task = "end"
            return state.result

        except Exception as e:
            logging.error(f"‚ùå Unexpected error in WebsiteSEOWriterAgent: {e}")
            state.result = "An error occurred during content generation."
            state.next_task = "end"
            return state.result

website_seo_writer_agent = WebsiteSEOWriterAgent()


class FacebookSEOWriterAgent:
    def write_seo(self, state):
        try:
            # üîé G·ªçi `vector_search()` ƒë·ªÉ l·∫•y d·ªØ li·ªáu
            context_data = vector_search(state.query)

            # ‚ùå N·∫øu kh√¥ng c√≥ k·∫øt qu·∫£, tr·∫£ v·ªÅ l·ªói
            if not context_data:
                logging.error("‚ùå Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ n√†o t·ª´ vector_search.")
                state.result = "Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ n√†o ph√π h·ª£p v·ªõi t·ª´ kh√≥a."
                state.next_task = "end"
                return state.result

            # üè∑Ô∏è L·∫•y th√¥ng tin t·ª´ context_data
            product_name = context_data.get("product_name", state.query) or "Kh√¥ng x√°c ƒë·ªãnh"
            topic_title = context_data.get("topic_title", f"Qu·∫£ng c√°o cho {product_name}")
            primary_keywords = context_data.get("primary_keywords", []) or [product_name]
            secondary_keywords = context_data.get("secondary_keywords", []) or []
            combined_chunks = context_data.get("chunk_texts", "Kh√¥ng c√≥ d·ªØ li·ªáu chi ti·∫øt.")

            # üîé Ki·ªÉm tra d·ªØ li·ªáu tr∆∞·ªõc khi g·ªçi API
            if not topic_title:
                logging.warning("‚ö†Ô∏è `topic_title` b·ªã thi·∫øu! D√πng m·∫∑c ƒë·ªãnh.")
                topic_title = f"Qu·∫£ng c√°o cho {product_name}"

            if not primary_keywords:
                logging.warning("‚ö†Ô∏è `primary_keywords` b·ªã thi·∫øu! D√πng m·∫∑c ƒë·ªãnh.")
                primary_keywords = [product_name]

            if not secondary_keywords:
                logging.warning("‚ö†Ô∏è `secondary_keywords` b·ªã thi·∫øu! D√πng m·∫∑c ƒë·ªãnh.")
                secondary_keywords = []

            if not combined_chunks:
                logging.warning("‚ö†Ô∏è `combined_chunks` tr·ªëng, n·ªôi dung c√≥ th·ªÉ b·ªã thi·∫øu.")
                combined_chunks = "Kh√¥ng c√≥ d·ªØ li·ªáu s·∫£n ph·∫©m chi ti·∫øt."

            # üîπ G·ªçi `generate_seo_tool` ƒë·ªÉ t·∫°o n·ªôi dung b√†i vi·∫øt Facebook
            response = generate_seo_tool.invoke({
                "product_name": product_name,
                "combined_chunks": combined_chunks,
                "topic_title": topic_title,
                "primary_keywords": primary_keywords,
                "secondary_keywords": secondary_keywords,
                "platform": "facebook"
            })

            # ‚úÖ ƒê·∫£m b·∫£o `state.context` kh√¥ng ph·∫£i `None`
            if state.context is None:
                state.context = {}

            # ‚úÖ L∆∞u b√†i vi·∫øt v√†o `state.context["last_facebook_post"]`
            state.context["last_facebook_post"] = {
                "product_name": product_name,
                "content": response
            }
            state.result = response
            state.next_task = "end"

            logging.info(f"‚úÖ B√†i vi·∫øt Facebook ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o state.context: {product_name}")

            return response

        except Exception as e:
            logging.error(f"‚ùå Unexpected error in FacebookSEOWriterAgent: {e}")
            state.result = "An error occurred during content generation."
            state.next_task = "end"
            return state.result

facebook_seo_writer_agent = FacebookSEOWriterAgent()

# Define Publisher Agent
import requests
import logging

class PublisherAgent:
    def publish_content(self, state):
        try:
            logging.info(f"üì¢ DEBUG - state.context tr∆∞·ªõc khi ƒëƒÉng: {state.context}")

            # ‚úÖ Ki·ªÉm tra n·∫øu `state.context` kh√¥ng c√≥ d·ªØ li·ªáu
            if not state.context or "last_facebook_post" not in state.context:
                logging.error("‚ùå Kh√¥ng c√≥ n·ªôi dung n√†o trong state.context ƒë·ªÉ ƒëƒÉng l√™n Facebook.")
                state.result = "Kh√¥ng c√≥ n·ªôi dung n√†o ƒë·ªÉ ƒëƒÉng l√™n Facebook. H√£y y√™u c·∫ßu t·∫°o n·ªôi dung tr∆∞·ªõc."
                state.next_task = "end"
                return state.result

            # ‚úÖ L·∫•y b√†i vi·∫øt cu·ªëi c√πng t·ª´ `FacebookSEOWriterAgent`
            last_facebook_post = state.context["last_facebook_post"]

            if not isinstance(last_facebook_post, dict) or "content" not in last_facebook_post:
                logging.error("‚ùå D·ªØ li·ªáu b√†i vi·∫øt kh√¥ng h·ª£p l·ªá!")
                state.result = "Kh√¥ng c√≥ n·ªôi dung n√†o ƒë·ªÉ ƒëƒÉng l√™n Facebook. H√£y y√™u c·∫ßu t·∫°o n·ªôi dung tr∆∞·ªõc."
                state.next_task = "end"
                return state.result

            # üîé Ki·ªÉm tra n·ªôi dung b√†i vi·∫øt
            post_content = last_facebook_post.get("content", "").strip()
            product_name = last_facebook_post.get("product_name", "Kh√¥ng x√°c ƒë·ªãnh")

            if not post_content:
                logging.error("‚ùå N·ªôi dung b√†i vi·∫øt r·ªóng, kh√¥ng th·ªÉ ƒëƒÉng l√™n Facebook.")
                state.result = "N·ªôi dung b√†i vi·∫øt r·ªóng, h√£y ki·ªÉm tra l·∫°i."
                state.next_task = "end"
                return state.result

            # ‚úÖ G·ª≠i b√†i ƒëƒÉng l√™n Facebook Page s·ª≠ d·ª•ng Graph API
            url = f"https://graph.facebook.com/{FACEBOOK_PAGE_ID}/feed"
            payload = {
                "message": post_content,
                "access_token": FACEBOOK_ACCESS_TOKEN
            }
            response = requests.post(url, data=payload)
            response_data = response.json()

            # üîé Ki·ªÉm tra ph·∫£n h·ªìi t·ª´ Facebook API
            if "id" in response_data:
                post_id = response_data["id"]
                logging.info(f"‚úÖ B√†i vi·∫øt '{product_name}' ƒë√£ ƒë∆∞·ª£c ƒëƒÉng th√†nh c√¥ng! Post ID: {post_id}")
                state.result = f"‚úÖ B√†i vi·∫øt '{product_name}' ƒë√£ ƒëƒÉng th√†nh c√¥ng! Xem t·∫°i: https://www.facebook.com/{post_id}"
            else:
                error_message = response_data.get("error", {}).get("message", "L·ªói kh√¥ng x√°c ƒë·ªãnh")
                logging.error(f"‚ùå L·ªói khi ƒëƒÉng b√†i: {error_message}")
                state.result = f"‚ùå L·ªói khi ƒëƒÉng b√†i: {error_message}"

            state.next_task = "end"
            return state.result

        except Exception as e:
            logging.error(f"‚ùå Unexpected error in PublisherAgent: {e}")
            state.result = "An error occurred during publishing."
            state.next_task = "end"
            return state.result
publisher_agent = PublisherAgent()

# L∆∞u tr·∫°ng th√°i global
global_state = None

def generate_seo():
    global global_state  # D√πng bi·∫øn to√†n c·ª•c ƒë·ªÉ gi·ªØ tr·∫°ng th√°i Supervisor gi·ªØa c√°c l·∫ßn g·ªçi

    data = request.json
    query = data.get('query', '')

    if not query:
        return jsonify({"error": "Query is required"}), 400

    try:
        # N·∫øu Supervisor ƒë√£ t·ªìn t·∫°i, gi·ªØ nguy√™n state v√† ch·ªâ c·∫≠p nh·∫≠t query
        if global_state is None:
            global_state = Supervisor()
            global_state.state = AgentState(query=query, next_task="start")
        else:
            global_state.state.query = query
            global_state.state.next_task = "start"

        # Ch·∫°y Supervisor
        global_state.run(query=query)
        
        if not global_state.state.result:
            return jsonify({"error": "Kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m ph√π h·ª£p."}), 404

        # Ki·ªÉm tra d·ªØ li·ªáu tr∆∞·ªõc khi tr·∫£ v·ªÅ
        supervisor_data = global_state.state.result
        print("Final Result Here", global_state.state.result)
        
        # X√≥a chu·ªói "```markdown" n·∫øu c√≥
        if supervisor_data and isinstance(supervisor_data, str):
            supervisor_data = supervisor_data.replace("```markdown", "").replace("```", "")

        return jsonify({"result": supervisor_data, "format": "markdown"}), 200
    except Exception as e:
        return jsonify({"error": str(e)}), 500

